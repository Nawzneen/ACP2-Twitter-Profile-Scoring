{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import call\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error, mean_absolute_percentage_error,\n",
    "                             mean_squared_log_error, median_absolute_error)\n",
    "from dataset_reader import get_dataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = get_dataset()\n",
    "print(f\"Dataset size: {df.shape[0]} rows\")\n",
    "df = df.drop('id', axis=1)\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset for test and train sets\n",
    "dataset_df = df.dropna()\n",
    "y_variable = \"credibility_score\"\n",
    "bin_count = 4\n",
    "random_state = 2\n",
    "all_features = True\n",
    "\n",
    "if bin_count and bin_count > 1:\n",
    "    y_values = dataset_df[y_variable]\n",
    "    min_y = np.amin(y_values)\n",
    "    max_y = np.amax(y_values)\n",
    "    bins_y = np.linspace(start=min_y, stop=max_y, num=bin_count)\n",
    "    print(f\"Bins: {bins_y}\")\n",
    "    binned_y = np.digitize(y, bins_y, right=True)\n",
    "    train_df, test_df = train_test_split(dataset_df, train_size=0.75, stratify=binned_y, random_state=random_state)\n",
    "else:\n",
    "    train_df, test_df = train_test_split(dataset_df, train_size=0.75, random_state=random_state)\n",
    "\n",
    "if all_features:\n",
    "    train_x = train_df.drop(y_variable, axis=1)\n",
    "    test_x = test_df.drop(y_variable, axis=1)\n",
    "else:\n",
    "    features = ['name_len', 'tweet_count', 'created_at', 'desc_len', 'desc_words',\n",
    "                'tweet.length', 'tweet.words', 'tweet.likes', 'tweet.mentions',\n",
    "                'tweet.sentiment.neg']\n",
    "    train_x = train_df[features]\n",
    "    test_x = test_df[features]\n",
    "\n",
    "train_y = train_df[y_variable].astype(float)\n",
    "test_y = test_df[y_variable].astype(float)\n",
    "\n",
    "print(f\"Training data rows: {train_df.shape[0]}\")\n",
    "print(f\"Test data rows: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=10, min_samples_leaf=3, random_state=random_state)\n",
    "#print(model.get_params())\n",
    "\n",
    "validate = True\n",
    "randomize_loo_pred = False\n",
    "\n",
    "if validate:\n",
    "    from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "    y_np = y.to_numpy()\n",
    "    y_std = np.std(y_np)\n",
    "    y_mean = np.sum(y_np) / len(y_np)\n",
    "    print(\"Score STD:\", y_std)\n",
    "    print(\"Score mean:\", y_mean)\n",
    "    errors = []\n",
    "    loo_df = dataset_df\n",
    "    for train_indices, test_index in LeaveOneOut().split(loo_df):\n",
    "        train_c = loo_df.iloc[train_indices]\n",
    "        train_cx = train_c.drop(y_variable, axis=1)\n",
    "        train_cy = train_c[y_variable].astype(float)\n",
    "        test_c = loo_df.iloc[test_index]\n",
    "        test_cx = test_c.drop(y_variable, axis=1)\n",
    "        test_cy = test_c[y_variable].astype(float)\n",
    "        model.fit(train_cx, train_cy)\n",
    "        if randomize_loo_pred:\n",
    "            prediction = np.random.normal(y_mean, y_std)\n",
    "        else:\n",
    "            prediction = model.predict(train_cx)[0]\n",
    "        expected = test_cy.iloc[0]\n",
    "        error = prediction - expected\n",
    "        errors.append(abs(error))\n",
    "\n",
    "    mean_error = sum(errors) / len(errors)\n",
    "    print(f\"LOO Mean error: {mean_error:.4f}\")\n",
    "    print(f\"LOO Max error: {np.amax(errors):.4f}\")\n",
    "\n",
    "    scores = cross_val_score(model, train_x, train_y, scoring=\"neg_mean_absolute_error\", cv=5)\n",
    "    scores = [abs(x) for x in scores]\n",
    "    avg_error = sum(scores) / len(scores)\n",
    "    print(f\"Kfold mean error: {avg_error:.4f}\")\n",
    "    print(f\"Kfold Max error: {np.amax(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectFromModel\n",
    "#sel = SelectFromModel(model)\n",
    "#model = sel.fit(train_x, train_y)\n",
    "#selected_feat = train_x.columns[(model.get_support())]\n",
    "#print(selected_feat)\n",
    "#print(train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate errors\n",
    "model.fit(train_x, train_y)\n",
    "prediction = model.predict(test_x)\n",
    "test_y_np = test_y.to_numpy()\n",
    "test_mae = mean_absolute_error(test_y_np, prediction)\n",
    "test_mdae = median_absolute_error(test_y_np, prediction)\n",
    "test_mse = mean_squared_error(test_y_np, prediction)\n",
    "test_msle = mean_squared_log_error(test_y_np, prediction)\n",
    "test_mape = mean_absolute_percentage_error(test_y_np, prediction)\n",
    "print(f\"Mean absolute error: {test_mae:.4f}\")\n",
    "print(f\"Median absolute error: {test_mdae:.4f}\")\n",
    "print(f\"Mean squared error: {test_mse:.4f}\")\n",
    "print(f\"Mean squared log error: {test_msle:.4f}\")\n",
    "print(f\"Mean absolute percentage error {test_mape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importances\n",
    "\n",
    "feature_names = list(train_df.columns)\n",
    "feature_names.remove(y_variable)\n",
    "\n",
    "# estimator = model.estimators_[5]\n",
    "# def get_feature_importances(estimator):\n",
    "#     importances = []\n",
    "#     for i, feature_name in enumerate(feature_names):\n",
    "#         importances.append(\n",
    "#             {\"importance\": estimator.feature_importances_[i], \"feature_name\": feature_name})\n",
    "#     importances = sorted(importances, key=lambda x: x[\"importance\"], reverse=True)\n",
    "#     return importances\n",
    "\n",
    "# importances = get_feature_importances(estimator)\n",
    "# importance_msg = \"\\n\".join(f\"{i['feature_name']}: {i['importance']:.3f}\" for i in importances)\n",
    "# print(importance_msg)\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# perm_imp = permutation_importance(\n",
    "#     model, test_x_scaled, test_y, n_repeats=10, n_jobs=2\n",
    "# )\n",
    "perm_imp = permutation_importance(\n",
    "    model, test_x, test_y, n_repeats=10, n_jobs=2\n",
    ")\n",
    "forest_importances = pd.Series(perm_imp.importances_mean, index=feature_names)\n",
    "forest_importances = forest_importances.sort_values()\n",
    "print(\"IMP:\" + str(forest_importances))\n",
    "print(\"STD: \" + str(perm_imp.importances_std))\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=perm_imp.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "\n",
    "# Uncomment to update saved model\n",
    "pickle.dump(model, open('model/model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tree\n",
    "\n",
    "estimator = model.estimators_[5]\n",
    "\n",
    "tmp_dir = os.path.join(os.getcwd(), 'tmp')\n",
    "if not os.path.isdir(tmp_dir):\n",
    "    os.mkdir(tmp_dir)\n",
    "graph_file = os.path.join(tmp_dir, 'tree.dot')\n",
    "png_file = os.path.join(tmp_dir, 'tree.png')\n",
    "\n",
    "export_graphviz(estimator, out_file=graph_file, feature_names=feature_names,\n",
    "                class_names=None, rounded=True, proportion=False, precision=2, filled=True)\n",
    "\n",
    "call(['dot', '-Tpng', graph_file, '-o', png_file, '-Gdpi=600'])\n",
    "\n",
    "Image(filename=png_file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bf1a5cf4aff157dbe1350d0c2c26794d3fa2cf7cb44058364317892fb1e2f69"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
